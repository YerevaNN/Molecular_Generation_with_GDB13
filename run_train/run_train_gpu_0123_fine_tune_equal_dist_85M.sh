

CUDA_VISIBLE_DEVICES="0,1,2,3" metaseq-train --task streaming_language_modeling ../Molecular_Generation_with_GDB13/data/data_bin_equal_dist_sf_4K/ --sample-break-mode "eos_pad_8" --hf-tokenizer ../Molecular_Generation_with_GDB13/data/tokenizers/tokenizer_sf/tokenizer.json --train-subset train --valid-subset valid --combine-valid-subsets --no-reshard-after-forward --use-sharded-state --checkpoint-activations --full-megatron-init --megatron-init-sigma 0.006 --activation-fn relu --arch transformer_lm --share-decoder-input-output-embed --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-attention-heads 12 --decoder-learned-pos --no-scale-embedding --dropout 0.0 --attention-dropout 0.0 --no-emb-dropout --weight-decay 0.1 --optimizer adam --adam-betas  "(0.9, 0.95)" --adam-eps 1e-08 --clip-norm 1.0 --clip-norm-type l2 --criterion cross_entropy --required-batch-size-multiple 1 --distributed-world-size 4 --model-parallel-size 1 --ddp-backend fully_sharded --memory-efficient-fp16 --fp16-init-scale 4 --fp16 --seed 1 --num-workers 0 --num-workers-valid 0 --lr-scheduler polynomial_decay --lr 6.00E-05 --end-learning-rate 6.00E-06 --warmup-updates 3 --total-num-update 31 --max-update 31 --tokens-per-sample 64 --batch-size 128 --update-freq 1 --log-format json --log-interval 1 --ignore-unused-valid-subsets --validate-interval-updates 10 --wandb-project Scaling_Laws --wandb-run-name OPT_85M_ep_1_equal_dist_sf_4K_6.00E-05 --save-interval-epochs 100 --keep-last-updates 1 --save-dir ./checkpoints/OPT_85M_ep_1_equal_dist_sf_4K_6.00E-05 --load-checkpoint-on-all-dp-ranks --finetune-from-model "./checkpoints/OPT_85M_ep_1_all_sf_848M_6.00E-04/checkpoint_last.pt"




    


CUDA_VISIBLE_DEVICES="0,1,2,3" metaseq-train --task streaming_language_modeling ../Molecular_Generation_with_GDB13/data/data_bin_equal_dist_sf_16K/ --sample-break-mode "eos_pad_8" --hf-tokenizer ../Molecular_Generation_with_GDB13/data/tokenizers/tokenizer_sf/tokenizer.json --train-subset train --valid-subset valid --combine-valid-subsets --no-reshard-after-forward --use-sharded-state --checkpoint-activations --full-megatron-init --megatron-init-sigma 0.006 --activation-fn relu --arch transformer_lm --share-decoder-input-output-embed --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-attention-heads 12 --decoder-learned-pos --no-scale-embedding --dropout 0.0 --attention-dropout 0.0 --no-emb-dropout --weight-decay 0.1 --optimizer adam --adam-betas  "(0.9, 0.95)" --adam-eps 1e-08 --clip-norm 1.0 --clip-norm-type l2 --criterion cross_entropy --required-batch-size-multiple 1 --distributed-world-size 4 --model-parallel-size 1 --ddp-backend fully_sharded --memory-efficient-fp16 --fp16-init-scale 4 --fp16 --seed 1 --num-workers 0 --num-workers-valid 0 --lr-scheduler polynomial_decay --lr 6.00E-05 --end-learning-rate 6.00E-06 --warmup-updates 13 --total-num-update 125 --max-update 125 --tokens-per-sample 64 --batch-size 128 --update-freq 1 --log-format json --log-interval 1 --ignore-unused-valid-subsets --validate-interval-updates 50 --wandb-project Scaling_Laws --wandb-run-name OPT_85M_ep_1_equal_dist_sf_16K_6.00E-05 --save-interval-epochs 100 --keep-last-updates 1 --save-dir ./checkpoints/OPT_85M_ep_1_equal_dist_sf_16K_6.00E-05 --load-checkpoint-on-all-dp-ranks --finetune-from-model "./checkpoints/OPT_85M_ep_1_all_sf_848M_6.00E-04/checkpoint_last.pt"




    


CUDA_VISIBLE_DEVICES="0,1,2,3" metaseq-train --task streaming_language_modeling ../Molecular_Generation_with_GDB13/data/data_bin_equal_dist_sf_64K/ --sample-break-mode "eos_pad_8" --hf-tokenizer ../Molecular_Generation_with_GDB13/data/tokenizers/tokenizer_sf/tokenizer.json --train-subset train --valid-subset valid --combine-valid-subsets --no-reshard-after-forward --use-sharded-state --checkpoint-activations --full-megatron-init --megatron-init-sigma 0.006 --activation-fn relu --arch transformer_lm --share-decoder-input-output-embed --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-attention-heads 12 --decoder-learned-pos --no-scale-embedding --dropout 0.0 --attention-dropout 0.0 --no-emb-dropout --weight-decay 0.1 --optimizer adam --adam-betas  "(0.9, 0.95)" --adam-eps 1e-08 --clip-norm 1.0 --clip-norm-type l2 --criterion cross_entropy --required-batch-size-multiple 1 --distributed-world-size 4 --model-parallel-size 1 --ddp-backend fully_sharded --memory-efficient-fp16 --fp16-init-scale 4 --fp16 --seed 1 --num-workers 0 --num-workers-valid 0 --lr-scheduler polynomial_decay --lr 6.00E-05 --end-learning-rate 6.00E-06 --warmup-updates 50 --total-num-update 500 --max-update 500 --tokens-per-sample 64 --batch-size 128 --update-freq 1 --log-format json --log-interval 1 --ignore-unused-valid-subsets --validate-interval-updates 100 --wandb-project Scaling_Laws --wandb-run-name OPT_85M_ep_1_equal_dist_sf_64K_6.00E-05 --save-interval-epochs 100 --keep-last-updates 1 --save-dir ./checkpoints/OPT_85M_ep_1_equal_dist_sf_64K_6.00E-05 --load-checkpoint-on-all-dp-ranks --finetune-from-model "./checkpoints/OPT_85M_ep_1_all_sf_848M_6.00E-04/checkpoint_last.pt"




    


CUDA_VISIBLE_DEVICES="0,1,2,3" metaseq-train --task streaming_language_modeling ../Molecular_Generation_with_GDB13/data/data_bin_equal_dist_sf_256K/ --sample-break-mode "eos_pad_8" --hf-tokenizer ../Molecular_Generation_with_GDB13/data/tokenizers/tokenizer_sf/tokenizer.json --train-subset train --valid-subset valid --combine-valid-subsets --no-reshard-after-forward --use-sharded-state --checkpoint-activations --full-megatron-init --megatron-init-sigma 0.006 --activation-fn relu --arch transformer_lm --share-decoder-input-output-embed --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-attention-heads 12 --decoder-learned-pos --no-scale-embedding --dropout 0.0 --attention-dropout 0.0 --no-emb-dropout --weight-decay 0.1 --optimizer adam --adam-betas  "(0.9, 0.95)" --adam-eps 1e-08 --clip-norm 1.0 --clip-norm-type l2 --criterion cross_entropy --required-batch-size-multiple 1 --distributed-world-size 4 --model-parallel-size 1 --ddp-backend fully_sharded --memory-efficient-fp16 --fp16-init-scale 4 --fp16 --seed 1 --num-workers 0 --num-workers-valid 0 --lr-scheduler polynomial_decay --lr 6.00E-05 --end-learning-rate 6.00E-06 --warmup-updates 200 --total-num-update 2000 --max-update 2000 --tokens-per-sample 64 --batch-size 128 --update-freq 1 --log-format json --log-interval 1 --ignore-unused-valid-subsets --validate-interval-updates 500 --wandb-project Scaling_Laws --wandb-run-name OPT_85M_ep_1_equal_dist_sf_256K_6.00E-05 --save-interval-epochs 100 --keep-last-updates 1 --save-dir ./checkpoints/OPT_85M_ep_1_equal_dist_sf_256K_6.00E-05 --load-checkpoint-on-all-dp-ranks --finetune-from-model "./checkpoints/OPT_85M_ep_1_all_sf_848M_6.00E-04/checkpoint_last.pt"




    


CUDA_VISIBLE_DEVICES="0,1,2,3" metaseq-train --task streaming_language_modeling ../Molecular_Generation_with_GDB13/data/data_bin_equal_dist_sf_1000K/ --sample-break-mode "eos_pad_8" --hf-tokenizer ../Molecular_Generation_with_GDB13/data/tokenizers/tokenizer_sf/tokenizer.json --train-subset train --valid-subset valid --combine-valid-subsets --no-reshard-after-forward --use-sharded-state --checkpoint-activations --full-megatron-init --megatron-init-sigma 0.006 --activation-fn relu --arch transformer_lm --share-decoder-input-output-embed --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-attention-heads 12 --decoder-learned-pos --no-scale-embedding --dropout 0.0 --attention-dropout 0.0 --no-emb-dropout --weight-decay 0.1 --optimizer adam --adam-betas  "(0.9, 0.95)" --adam-eps 1e-08 --clip-norm 1.0 --clip-norm-type l2 --criterion cross_entropy --required-batch-size-multiple 1 --distributed-world-size 4 --model-parallel-size 1 --ddp-backend fully_sharded --memory-efficient-fp16 --fp16-init-scale 4 --fp16 --seed 1 --num-workers 0 --num-workers-valid 0 --lr-scheduler polynomial_decay --lr 6.00E-05 --end-learning-rate 6.00E-06 --warmup-updates 781 --total-num-update 7813 --max-update 7813 --tokens-per-sample 64 --batch-size 128 --update-freq 1 --log-format json --log-interval 1 --ignore-unused-valid-subsets --validate-interval-updates 500 --wandb-project Scaling_Laws --wandb-run-name OPT_85M_ep_1_equal_dist_sf_1000K_6.00E-05 --save-interval-epochs 100 --keep-last-updates 1 --save-dir ./checkpoints/OPT_85M_ep_1_equal_dist_sf_1000K_6.00E-05 --load-checkpoint-on-all-dp-ranks --finetune-from-model "./checkpoints/OPT_85M_ep_1_all_sf_848M_6.00E-04/checkpoint_last.pt"


